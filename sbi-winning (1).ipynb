{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10829757,"sourceType":"datasetVersion","datasetId":6724597}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:10.353803Z","iopub.execute_input":"2025-02-23T09:52:10.354101Z","iopub.status.idle":"2025-02-23T09:52:10.665709Z","shell.execute_reply.started":"2025-02-23T09:52:10.354064Z","shell.execute_reply":"2025-02-23T09:52:10.664966Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/zaruri-data/validation.json\n/kaggle/input/zaruri-data/train.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# pip install transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:11.658310Z","iopub.execute_input":"2025-02-23T09:52:11.658737Z","iopub.status.idle":"2025-02-23T09:52:11.662228Z","shell.execute_reply.started":"2025-02-23T09:52:11.658709Z","shell.execute_reply":"2025-02-23T09:52:11.661297Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata_files = {\n    \"train\": \"/kaggle/input/zaruri-data/train.json\", #Path to the train data json\n    \"validation\": \"/kaggle/input/zaruri-data/validation.json\" #Path to validation data json\n}\ndataset = load_dataset(\"json\", data_files=data_files)\nprint(dataset)\n# Should now show 2 splits: train/validation with multiple rows each.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:12.110981Z","iopub.execute_input":"2025-02-23T09:52:12.111280Z","iopub.status.idle":"2025-02-23T09:52:13.883131Z","shell.execute_reply.started":"2025-02-23T09:52:12.111259Z","shell.execute_reply":"2025-02-23T09:52:13.882487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd8ba8855fd4a2899b8da70ca91ceda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b8f06f662a4bef828e0dd5a7103aa3"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 520\n    })\n    validation: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 67\n    })\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = \"t5-small\"  # or \"google/flan-t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:13.884254Z","iopub.execute_input":"2025-02-23T09:52:13.884564Z","iopub.status.idle":"2025-02-23T09:52:35.408435Z","shell.execute_reply.started":"2025-02-23T09:52:13.884536Z","shell.execute_reply":"2025-02-23T09:52:35.407518Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075915b644ac45d0984e4806fcadd0ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2050354d2c54dbea6b6123b86c1109d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e329d674c104f1cad0716b1ab6a7b81"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4103f965232048aaa15fbe27769557e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7296701d16f4631ac11fd98f61215aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107f80db263d4c4e859fdc8365328188"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [\"question: \" + q for q in examples[\"question\"]]\n    targets = [ans for ans in examples[\"answer\"]]\n    \n    # Tokenize question\n    model_inputs = tokenizer(inputs, truncation=True)\n    \n    # Tokenize answer\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, truncation=True)\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:35.410285Z","iopub.execute_input":"2025-02-23T09:52:35.410940Z","iopub.status.idle":"2025-02-23T09:52:35.416270Z","shell.execute_reply.started":"2025-02-23T09:52:35.410907Z","shell.execute_reply":"2025-02-23T09:52:35.415485Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:35.417430Z","iopub.execute_input":"2025-02-23T09:52:35.417625Z","iopub.status.idle":"2025-02-23T09:52:35.639253Z","shell.execute_reply.started":"2025-02-23T09:52:35.417609Z","shell.execute_reply":"2025-02-23T09:52:35.638393Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79c28ce45c94dcea6f023290cd62129"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f6e129ae3b43818bcc8e7aaa59c3f0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:35.640001Z","iopub.execute_input":"2025-02-23T09:52:35.640233Z","iopub.status.idle":"2025-02-23T09:52:35.682704Z","shell.execute_reply.started":"2025-02-23T09:52:35.640204Z","shell.execute_reply":"2025-02-23T09:52:35.682144Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_t5_chatbot\",\n    \n    # Evaluate every epoch (so we can compare metrics and decide \"best\")\n    eval_strategy=\"epoch\",\n    \n    # Save a checkpoint every epoch (so we can pick the best one)\n    save_strategy=\"epoch\",\n    \n    # Load the best model at the end of training\n    load_best_model_at_end=True,\n    \n    # Metric to decide \"best\" model (e.g. \"eval_loss\")\n    # or any custom metric you set up in compute_metrics()\n    metric_for_best_model=\"eval_loss\",\n    \n    # If 'eval_loss' is the metric, lower is better\n    greater_is_better=False,\n    \n    # Limit how many checkpoints are kept\n    # so you donâ€™t clutter disk space\n    save_total_limit=1,\n    \n    num_train_epochs=25,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    learning_rate=1e-4,\n    logging_steps=50,\n    report_to=\"none\"  # or \"tensorboard\", etc.\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:35.683505Z","iopub.execute_input":"2025-02-23T09:52:35.683757Z","iopub.status.idle":"2025-02-23T09:52:35.866449Z","shell.execute_reply.started":"2025-02-23T09:52:35.683727Z","shell.execute_reply":"2025-02-23T09:52:35.865657Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:35.867323Z","iopub.execute_input":"2025-02-23T09:52:35.867637Z","iopub.status.idle":"2025-02-23T09:52:37.835291Z","shell.execute_reply.started":"2025-02-23T09:52:35.867608Z","shell.execute_reply":"2025-02-23T09:52:37.834561Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!rm -r /kaggle/working/my_t5_chatbot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:37.837132Z","iopub.execute_input":"2025-02-23T09:52:37.837341Z","iopub.status.idle":"2025-02-23T09:52:37.980921Z","shell.execute_reply.started":"2025-02-23T09:52:37.837323Z","shell.execute_reply":"2025-02-23T09:52:37.979873Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(\"my_t5_chatbot\")\ntokenizer.save_pretrained(\"my_t5_chatbot\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:37.982300Z","iopub.execute_input":"2025-02-23T09:52:37.982550Z","iopub.status.idle":"2025-02-23T09:57:19.581811Z","shell.execute_reply.started":"2025-02-23T09:52:37.982529Z","shell.execute_reply":"2025-02-23T09:57:19.581053Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1625' max='1625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1625/1625 04:38, Epoch 25/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.621100</td>\n      <td>2.494319</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.058700</td>\n      <td>2.206805</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.864400</td>\n      <td>2.051885</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.492100</td>\n      <td>1.946608</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.437900</td>\n      <td>1.872376</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.274600</td>\n      <td>1.804032</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.118800</td>\n      <td>1.753674</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.108600</td>\n      <td>1.719790</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.965200</td>\n      <td>1.698892</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.920300</td>\n      <td>1.659948</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.842200</td>\n      <td>1.624767</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.860200</td>\n      <td>1.611192</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.779700</td>\n      <td>1.592411</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.673800</td>\n      <td>1.584106</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.716700</td>\n      <td>1.564795</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.659900</td>\n      <td>1.561043</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.597700</td>\n      <td>1.543332</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.625700</td>\n      <td>1.534794</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.555500</td>\n      <td>1.537830</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.569000</td>\n      <td>1.531167</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.560200</td>\n      <td>1.525593</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.583400</td>\n      <td>1.525685</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.555700</td>\n      <td>1.523502</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.520200</td>\n      <td>1.524198</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.536200</td>\n      <td>1.523373</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('my_t5_chatbot/tokenizer_config.json',\n 'my_t5_chatbot/special_tokens_map.json',\n 'my_t5_chatbot/spiece.model',\n 'my_t5_chatbot/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the fine-tuned T5 model and tokenizer\nmodel_path = \"my_t5_chatbot\"\ntokenizer = T5Tokenizer.from_pretrained(model_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\n\ndef generate_answer(question, max_length=250, num_beams=10):\n    \"\"\"\n    Generate a coherent and concise answer using the fine-tuned T5 model.\n    \n    Args:\n    - question (str): User's question.\n    - max_length (int): Max length for generated answers.\n    - num_beams (int): Number of beams for beam search.\n    \n    Returns:\n    - str: The chatbot's answer.\n    \"\"\"\n    input_text = f\"question: {question}\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n    \n    # Generate response with improved decoding options\n    outputs = model.generate(\n        input_ids, \n        max_length=max_length, \n        num_beams=num_beams, \n        early_stopping=True,\n        repetition_penalty=2.0,  # Stronger penalty to avoid repetitive phrases\n        length_penalty=1.2,       # Encourage slightly longer, meaningful responses\n        no_repeat_ngram_size=3,   # Prevent repeating n-grams (like 'non-participating' spam)\n        temperature=0.9,          # Add randomness for more human-like variety\n        top_k=50,                 # Consider top 50 tokens at each step\n        top_p=0.9                 # Nucleus sampling for diverse outputs\n    )\n    \n    # Decode and return the generated answer\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example usage\nuser_question = \"What is Retire SMart Plus?\"\nprint(\"User Question:\", user_question)\nprint(\"Chatbot Answer:\", generate_answer(user_question))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:57:19.582676Z","iopub.execute_input":"2025-02-23T09:57:19.582973Z","iopub.status.idle":"2025-02-23T09:57:21.573140Z","shell.execute_reply.started":"2025-02-23T09:57:19.582944Z","shell.execute_reply":"2025-02-23T09:57:21.572300Z"}},"outputs":[{"name":"stdout","text":"User Question: What is Retire SMart Plus?\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Chatbot Answer: Retire SMart Plus is a non-linked, non-participating retirement plan designed to help you build your retirement corpus.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"user_question = \"What are the different types of insurance policies offered by SBI Life?\"\nprint(\"User Question:\", user_question)\nprint(\"Chatbot Answer:\", generate_answer(user_question))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:57:21.573943Z","iopub.execute_input":"2025-02-23T09:57:21.574201Z","iopub.status.idle":"2025-02-23T09:57:22.962223Z","shell.execute_reply.started":"2025-02-23T09:57:21.574179Z","shell.execute_reply":"2025-02-23T09:57:22.961328Z"}},"outputs":[{"name":"stdout","text":"User Question: What are the different types of insurance policies offered by SBI Life?\nChatbot Answer: SBI Life offers a wide range of insurance policies, starting from the 1st policy year, to the end of the policy term.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import shutil\n\n# Zip the folder\nshutil.make_archive('/kaggle/working/my_t5_chatbot', 'zip', '/kaggle/working/my_t5_chatbot')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:57:22.963282Z","iopub.execute_input":"2025-02-23T09:57:22.963644Z","iopub.status.idle":"2025-02-23T09:58:09.760642Z","shell.execute_reply.started":"2025-02-23T09:57:22.963605Z","shell.execute_reply":"2025-02-23T09:58:09.759872Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/my_t5_chatbot.zip'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}